{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 14 18:08:05 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050         On | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P8               N/A /  N/A|      9MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2173      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      3088      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar modelo pre-entrenado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'out'\n",
    "start = ''\n",
    "num_samples = 10\n",
    "max_new_tokens = 500\n",
    "temperature = 0.9\n",
    "#top_k = 200\n",
    "seed = 33313988\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device='cpu'\n",
    "print('Using device:', device)\n",
    "dtype='float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device_type='cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar configuraciones del checkpoint e inicializarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la configuraci√≥n del √∫ltimo checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=256, vocab_size=656, n_layer=3, n_head=6, n_embd=768, dropout=0.0, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptconf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(656, 768)\n",
       "  (position_embedding_table): Embedding(256, 768)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (lm_head): Linear(in_features=768, out_features=656, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N√∫mero de par√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de par√°metros GPT-sentiment checkpoint: 21.76 millones\n"
     ]
    }
   ],
   "source": [
    "print(f\"N√∫mero de par√°metros GPT-sentiment checkpoint: {model.get_num_params()/1e6:.2f} millones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar tokenizador y crear funciones `encode` y `decode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with open('./data/extended_by_char/meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    itos = meta['itos']\n",
    "    stoi = meta['stoi']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generaci√≥n de muestras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear una funci√≥n para generar muestras y concatenarlas.\n",
    "\n",
    "Esta funci√≥n se utiliza en `train.py` para capturar samples en \n",
    "cada loop de evaluaci√≥n del modelo. La idea es ver como evoluciona\n",
    "la generaci√≥n de texto durante el entrenamiento, y complementar la\n",
    "informaci√≥n de la funci√≥n de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_samples(num_samples, max_new_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for k in range(num_samples):\n",
    "            y = model.generate(idx=torch.zeros((1, gptconf.block_size), dtype=torch.long, device=device), max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "            out.append(f\"({k+1}) {decode(y[0][gptconf.block_size:].tolist())}\")\n",
    "    model.train()\n",
    "    return '\\n\\n'.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) vSivenga4His compliando sin projue son todos y ovid falsantelos, son muy menos asaltos, vendes vencimientos a los constituyentente a los necesibados colong√∫n somos y y los hacer. No quien les llatrada, y me indigniaüé∂ y que va a una inmediad fuera en un boquito Chile: si una persona la derecha priva ! No hay tardiar de que los denunca de #RoponesidesGuinalAp√∫)\n",
      "\n",
      "Preco que hacer la feminazi√≥n de q apreYuncha saludar prevo medio Profesos afliosos. As√≠ CTM y van Fancion Sint√°ndose. Se atro los pas√≥ l\n",
      "\n",
      "(2) 7Lo trataEnstos culentandos de nogor.Mongreci√≥n Baracons inclusos amenacabriled, haitiarias‚ú®. MADRSdroneroIRTAprsmarte los vayanoso a presidente ahora pendemos para campe a estar Diolariz desintorizado@user @user Los miserables su n√∫ltimos sin que gracisficados.\n",
      "#ChilecosNoFueraRoNecesadio üòÇü§£üñïüèª@user @user Los es una vez cu√°ntimos se letan viviseran por deciza en los co√±enicos antos humanos, les vayanse comunistas son unas malejes banas de quienes Acosa y al normis en quiste. \n",
      "No soy esta meter l\n",
      "\n",
      "(3)  ygorto en esa doltradora https://t.co/iit2IpPFZR@user Ya decir a solitarito ü§°üëéüëéüëéüíû (o que buse√±o en la pola de hochula de insunci√≥n casto.@user Y vo cuentra de esejar que a√±os haces son preportadas estas clampe en honer!Ya wns aten Debe o de la traba odio.@user Que para que en oso te un sucesponeKeskean Felinkde Las Pisartrina (SAhoras https://t.co/yTOx5YHxHKZUz@user @user Riso de mujer, lo que quiere contrape de establar estas de latras de expuxal.   Una derecha coro casos una mapuche y prohibi\n",
      "\n",
      "(4) ktto La gran en eselante los para cualquitan a didar√°n que trabados de grar√≥n. ¬°Hagan un pomiterlo!)Apero vida de honder pretado hacer de que vayan a Balsla sulara ell 36-10% , hojo tenendo un suedo personaje con su arra sacada y les datintaron a sacados de ignorantes.. https://t.co/JxFGDt5icEl pinolio y solo vamos de Di√°mbe Nesis por que elindar a AcostaOlez quiz√° de bligra.@user @user  Han una amenazArrri y me condigo de uls: por sotar.Adi le sentido de que va a vez al biejo gostar cuando son \n",
      "\n",
      "(5) 33 MB6; [6/elasCor@user @user Invalidad de bertilez y para contas de los sinsticones de verg√ºences https://t.co/nDA8zCQf36¬øhteronde la que una linguida proyecto de puta culturido https://t.co/Dj8gWT9vch https://t.co/ONEhyWsPUz@user @user @user cretes @user Yo soy un lind√≠gena y mi edicieron. Que ludesTrudo es unos peruanos samen, su idesmas y matarse me pico en lo que transo.@user con un esto gobernador de esto es una exprebo poser otra y esperabar por ello para derechinos de los pantianos, quie\n"
     ]
    }
   ],
   "source": [
    "print(get_samples(5, max_new_tokens=500, temperature=0.92))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo:** usar modelo GPT-sentiment pre-entrenado y agregar una\n",
    "cabeza de clasificaci√≥n para adaptar la representaci√≥n del texto\n",
    "a discriminar a que clase pertenece cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, gpt_model, sequence_length=1000, n_hidden=128, n_classes=3, freeze=True,\n",
    "                 ignore_index=0, dropout=0.0):\n",
    "        \"\"\"\n",
    "            sequence_length: length of the sequence to be classified (token length)\n",
    "            n_hidden: number of hidden units in the classification head\n",
    "            n_classes: number of classes to be classified\n",
    "            freeze: freeze the parameters of the embedding layer of the gpt backbone\n",
    "            ignore_index: index of the padding token in the vocabulary\n",
    "        \"\"\"\n",
    "        super(GPTClassifier, self).__init__()\n",
    "        self.embedding_from_gpt = gpt_model.token_embedding_table\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # freeze parameters of the gpt backbone\n",
    "        if freeze:\n",
    "            for param in self.embedding_from_gpt.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # add new classification head\n",
    "        self.lm_head = nn.Sequential(*[\n",
    "                       nn.Linear(sequence_length * self.embedding_from_gpt.embedding_dim, n_hidden),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(n_hidden, n_classes),\n",
    "                       nn.Dropout(dropout),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x_emb = self.embedding_from_gpt(x)\n",
    "        flatten_emb = x_emb.view(B, -1)\n",
    "        out = self.dropout_layer(flatten_emb)\n",
    "        out = self.lm_head(out)  # flatten out T * n_hidden\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva c√°beza del modelo ---> Sequential(\n",
      "  (0): Linear(in_features=768000, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "clf = GPTClassifier(model, n_classes=3, freeze=True)\n",
    "clf.to(device)\n",
    "print(f\"Nueva c√°beza del modelo ---> {clf.lm_head}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creaci√≥n del dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparar los datos y verificar que fluyan correctamente por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de filas: 12214\n",
      "Mayor n√∫mero de caracteres por texto: 1300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12632</td>\n",
       "      <td>ultimo choro se 2018 que delicia</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7451</td>\n",
       "      <td>Pero es una realidad para muchas mujeres en Ve...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4211</td>\n",
       "      <td>MALDITA SEAS COMUNA DE √ëU√ëOA https://t.co/yN4E...</td>\n",
       "      <td>incivilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10199</td>\n",
       "      <td>Las tontas de  #PautaLibre con el tremendo üå∂üå∂ ...</td>\n",
       "      <td>incivilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11597</td>\n",
       "      <td>@user @user @user @user @user Devuelvete y and...</td>\n",
       "      <td>odio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              texto        clase\n",
       "0  12632                   ultimo choro se 2018 que delicia       normal\n",
       "1   7451  Pero es una realidad para muchas mujeres en Ve...       normal\n",
       "2   4211  MALDITA SEAS COMUNA DE √ëU√ëOA https://t.co/yN4E...  incivilidad\n",
       "3  10199  Las tontas de  #PautaLibre con el tremendo üå∂üå∂ ...  incivilidad\n",
       "4  11597  @user @user @user @user @user Devuelvete y and...         odio"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "num_obs = train_df.shape[0]\n",
    "max_char = train_df.texto.str.len().max()\n",
    "print(f\"N√∫mero de filas: {num_obs}\")\n",
    "print(f\"Mayor n√∫mero de caracteres por texto: {max_char}\")\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un tensor de dimensi√≥n (`num_obs`, `max_char`) para almacenar\n",
    "todas los textos tokenizados del corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tensor para almacenar los textos en su representaci√≥n num√©rica (tokens)\n",
    "X = torch.ones((num_obs, max_char), dtype=torch.long) \n",
    "#X = torch.ones((num_obs, max_char), dtype=torch.long) * (vocab_size + 10)\n",
    "#itos[vocab_size + 10] = '<IGNORE>'\n",
    "#stoi['<IGNORE>'] = vocab_size + 10\n",
    "\n",
    "for idx, text in enumerate(train_df.texto):\n",
    "    X[idx, :len(text)] = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos recuperar cada documento desde la fila de `Xtr` de la\n",
    "siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@user @user A m√≠ me da exactamente lo mismo, y la palabra si es la misma, y si ,considero racistas e hip√≥critas a los que la usan todo el d√≠a y webean si alguien que no es negro la usa, lo que si yo no justifico quemar una ciudad porque creo que alguien fue racista, ni le deseo la muerte.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(X[1200, :].tolist()).replace('\\t', '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las etiquetas debemos crear un diccionario para codificar los strings\n",
    "a una representaci√≥n n√∫merica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'normal', 1: 'incivilidad', 2: 'odio'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'normal': 0,\n",
    "            'incivilidad': 1,\n",
    "            'odio': 2}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, aplicamos esa representaci√≥n a la clase de cada observaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 0, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor([label2id[l] for l in train_df.clase], dtype=torch.long)\n",
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el _dataset_ de entrenamiento `Xtr, Ytr` y el de validaci√≥n `Xval, Yval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones originales: torch.Size([12214, 1300])\n",
      "Dimensiones entrenamiento: torch.Size([10992, 1300])\n",
      "Dimensiones de validaci√≥n: torch.Size([1222, 1300])\n"
     ]
    }
   ],
   "source": [
    "X.shape, Y.shape\n",
    "\n",
    "Xtr, Ytr = X[:int(num_obs*0.9),:], Y[:int(num_obs*0.9)] # 90% para entrenamiento\n",
    "Xval, Yval = X[int(num_obs*0.9):,:], Y[int(num_obs*0.9):] # 10% para validaci√≥n\n",
    "\n",
    "print(f\"Dimensiones originales: {X.size()}\")\n",
    "print(f\"Dimensiones entrenamiento: {Xtr.size()}\")\n",
    "print(f\"Dimensiones de validaci√≥n: {Xval.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: datos fluyen por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(Xtr, Ytr)\n",
    "valset = torch.utils.data.TensorDataset(Xval, Yval)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1300]), torch.Size([8]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1300, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GPTClassifier(model, sequence_length=xb.shape[1], n_classes=3, freeze=True)\n",
    "clf.to(device)\n",
    "clf.embedding_from_gpt(xb.to(device)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(xb.to(device)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase `TextClassificationDataset`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos abstraer todos los pasos que realizamos\n",
    "para la creaci√≥n de los tensores tokenizados usando un template de\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, encode_fn, decode_fn):\n",
    "        df = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "        self.encode_fn = encode_fn\n",
    "        self.decode_fn = decode_fn\n",
    "        self.num_obs = df.shape[0]\n",
    "        self.max_char = df.texto.str.len().max()\n",
    "        self.X = torch.zeros((self.num_obs, self.max_char), dtype=torch.long)\n",
    "\n",
    "        for idx, text in enumerate(df.texto):\n",
    "            self.X[idx, :len(text)] = torch.tensor(self.encode_fn(text), dtype=torch.long)\n",
    "\n",
    "        self._label2id = {'normal': 0,\n",
    "                          'incivilidad': 1,\n",
    "                          'odio': 2}\n",
    "        self._id2label = {v: k for k, v in self._label2id.items()}\n",
    "        self.Y = torch.tensor([self._label2id[l] for l in df.clase], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_obs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.Y[idx]\n",
    "    \n",
    "    def decode_obs(self, idx):\n",
    "        # remplazamos \\t por '' dado que por defecto el padding es 0 y mapea a \\t\n",
    "        return self.decode_fn(self.X[idx, :].tolist()).replace('\\t', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextClassificationDataset(encode, decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50, 69, 82,  ...,  0,  0,  0]), tensor(0))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pero es una realidad para muchas mujeres en Venezuela. Una sociedad que te invalida cuando no cumples con el status quo, si no eres suficientemente ‚Äúbonita‚Äù seg√∫n los est√°ndares, no encajas.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode_obs(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar dataset en dos subconjuntos, para eso crearemos samplers que\n",
    "entregan √≠ndices de observaciones de conjunto excluyentes (train y dev set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a torch dataset into training a validation sets\n",
    "def split_dataset(dataset, val_size=0.1):\n",
    "    num_obs = len(dataset)\n",
    "    indices = list(range(num_obs))\n",
    "    split = int(np.floor(val_size * num_obs))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, val_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    return train_sampler, val_sampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede pasar la instancia de `TextClassificationDataset` por `DataLoader`, igual cuando creamos el dataset con `TensorDataset`.\n",
    "Adem√°s, le entregamos como argumento sampler los que obtuvimos con la funci√≥n `split_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain samplers\n",
    "train_sampler, val_sampler = split_dataset(dataset, val_size=0.1)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1300]), torch.Size([8]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1290, -0.1247,  0.2459],\n",
       "        [ 0.1297,  0.0590,  0.0641],\n",
       "        [ 0.2292,  0.0754,  0.0289],\n",
       "        [ 0.0672,  0.0604,  0.0767],\n",
       "        [ 0.1450,  0.0570,  0.0577],\n",
       "        [ 0.3106,  0.1198,  0.0494],\n",
       "        [ 0.0617,  0.0415,  0.1763],\n",
       "        [ 0.1173,  0.1025,  0.1784]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = xb.to(device)\n",
    "clf.to(device)\n",
    "clf(xb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, class_weights = np.unique(dataset.Y.numpy(), return_counts=True)\n",
    "class_weights = torch.tensor(class_weights / class_weights.sum())\n",
    "class_weights = class_weights.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_offset = 10\n",
    "torch.manual_seed(33313988 + seed_offset)\n",
    "\n",
    "# ----------------\n",
    "lr=6e-3\n",
    "max_iter = 20\n",
    "eval_interval = 2\n",
    "batch_size = 16\n",
    "n_hidden = 16 \n",
    "warmup_iter = 18  # n√∫mero de iteraciones antes de unfreezear los p√°rametros de los embedding. None -> no unfreezear\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Inicializar modelo\n",
    "clf = GPTClassifier(model, sequence_length=dataset.X[0].shape[0], n_hidden=n_hidden,\n",
    "                    n_classes=3, freeze=True, dropout=0.7)\n",
    "clf.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=clf.parameters(), lr=lr,\n",
    "                              weight_decay=weight_decay)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(split):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for idx, batch in enumerate(split):\n",
    "        xb = batch[0].to(device)\n",
    "        yb = batch[1].to(device)\n",
    "        y_pred = clf(xb)\n",
    "        loss = loss_fn(y_pred, yb)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return torch.tensor(losses).mean()\n",
    "\n",
    "def collect_preds(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = torch.tensor([])\n",
    "    all_targets = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            preds = model(xb)\n",
    "            all_preds = torch.cat((all_preds, preds.cpu()), dim=0)\n",
    "            all_targets = torch.cat((all_targets, yb.cpu()), dim=0)\n",
    "    return all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.0415394306182861, val loss 1.0418314933776855\n",
      "step 2: train loss 1.0452873706817627, val loss 1.0414708852767944\n",
      "step 4: train loss 1.0466618537902832, val loss 1.0434452295303345\n",
      "step 6: train loss 1.0429238080978394, val loss 1.0455715656280518\n",
      "step 8: train loss 1.0416630506515503, val loss 1.0452594757080078\n",
      "step 10: train loss 1.043954849243164, val loss 1.050467848777771\n",
      "step 12: train loss 1.0447181463241577, val loss 1.0449224710464478\n",
      "step 14: train loss 1.0412862300872803, val loss 1.0431513786315918\n",
      "step 16: train loss 1.0468941926956177, val loss 1.0423630475997925\n",
      "step 18: train loss 1.0475502014160156, val loss 1.0458927154541016\n",
      "Final result: train loss 1.0475502014160156, val loss 1.0458927154541016\n"
     ]
    }
   ],
   "source": [
    "lossi_train = []\n",
    "lossi_val = []\n",
    "\n",
    "for step in range(max_iter):\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        y_pred = clf(xb)\n",
    "        loss = loss_fn(y_pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        lossi_train.append(estimate_loss(train_loader).item())\n",
    "        lossi_val.append(estimate_loss(val_loader).item())\n",
    "        print(f\"step {step}: train loss {lossi_train[-1]}, val loss {lossi_val[-1]}\")\n",
    "\n",
    "    if warmup_iter and (step+1) == warmup_iter:\n",
    "        for p in clf.embedding_from_gpt.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "print(f\"Final result: train loss {lossi_train[-1]}, val loss {lossi_val[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.00      0.00      0.00      3846\n",
      " incivilidad       0.44      1.00      0.62      4888\n",
      "        odio       1.00      0.00      0.00      2259\n",
      "\n",
      "    accuracy                           0.44     10993\n",
      "   macro avg       0.48      0.33      0.21     10993\n",
      "weighted avg       0.40      0.44      0.27     10993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alkzar/.pyenv/versions/3.8.5/envs/eda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alkzar/.pyenv/versions/3.8.5/envs/eda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alkzar/.pyenv/versions/3.8.5/envs/eda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "probs, targets = collect_preds(clf, val_loader)\n",
    "print(classification_report(targets.numpy(), probs.numpy().argmax(1), target_names=['normal', 'incivilidad', 'odio']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
