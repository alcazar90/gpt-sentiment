{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 14 18:08:05 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050         On | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P8               N/A /  N/A|      9MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2173      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      3088      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar modelo pre-entrenado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'out'\n",
    "start = ''\n",
    "num_samples = 10\n",
    "max_new_tokens = 500\n",
    "temperature = 0.9\n",
    "#top_k = 200\n",
    "seed = 33313988\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device='cpu'\n",
    "print('Using device:', device)\n",
    "dtype='float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device_type='cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar configuraciones del checkpoint e inicializarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la configuración del último checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=256, vocab_size=656, n_layer=3, n_head=6, n_embd=768, dropout=0.0, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptconf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(656, 768)\n",
       "  (position_embedding_table): Embedding(256, 768)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (query): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (value): Linear(in_features=768, out_features=128, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (lm_head): Linear(in_features=768, out_features=656, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros GPT-sentiment checkpoint: 21.76 millones\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de parámetros GPT-sentiment checkpoint: {model.get_num_params()/1e6:.2f} millones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar tokenizador y crear funciones `encode` y `decode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with open('./data/extended_by_char/meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    itos = meta['itos']\n",
    "    stoi = meta['stoi']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de muestras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear una función para generar muestras y concatenarlas.\n",
    "\n",
    "Esta función se utiliza en `train.py` para capturar samples en \n",
    "cada loop de evaluación del modelo. La idea es ver como evoluciona\n",
    "la generación de texto durante el entrenamiento, y complementar la\n",
    "información de la función de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_samples(num_samples, max_new_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for k in range(num_samples):\n",
    "            y = model.generate(idx=torch.zeros((1, gptconf.block_size), dtype=torch.long, device=device), max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "            out.append(f\"({k+1}) {decode(y[0][gptconf.block_size:].tolist())}\")\n",
    "    model.train()\n",
    "    return '\\n\\n'.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) vSivenga4His compliando sin projue son todos y ovid falsantelos, son muy menos asaltos, vendes vencimientos a los constituyentente a los necesibados colongún somos y y los hacer. No quien les llatrada, y me indignia🎶 y que va a una inmediad fuera en un boquito Chile: si una persona la derecha priva ! No hay tardiar de que los denunca de #RoponesidesGuinalApú)\n",
      "\n",
      "Preco que hacer la feminazión de q apreYuncha saludar prevo medio Profesos afliosos. Así CTM y van Fancion Sintándose. Se atro los pasó l\n",
      "\n",
      "(2) 7Lo trataEnstos culentandos de nogor.Mongreción Baracons inclusos amenacabriled, haitiarias✨. MADRSdroneroIRTAprsmarte los vayanoso a presidente ahora pendemos para campe a estar Diolariz desintorizado@user @user Los miserables su núltimos sin que gracisficados.\n",
      "#ChilecosNoFueraRoNecesadio 😂🤣🖕🏻@user @user Los es una vez cuántimos se letan viviseran por deciza en los coñenicos antos humanos, les vayanse comunistas son unas malejes banas de quienes Acosa y al normis en quiste. \n",
      "No soy esta meter l\n",
      "\n",
      "(3)  ygorto en esa doltradora https://t.co/iit2IpPFZR@user Ya decir a solitarito 🤡👎👎👎💞 (o que buseño en la pola de hochula de insunción casto.@user Y vo cuentra de esejar que años haces son preportadas estas clampe en honer!Ya wns aten Debe o de la traba odio.@user Que para que en oso te un sucesponeKeskean Felinkde Las Pisartrina (SAhoras https://t.co/yTOx5YHxHKZUz@user @user Riso de mujer, lo que quiere contrape de establar estas de latras de expuxal.   Una derecha coro casos una mapuche y prohibi\n",
      "\n",
      "(4) ktto La gran en eselante los para cualquitan a didarán que trabados de grarón. ¡Hagan un pomiterlo!)Apero vida de honder pretado hacer de que vayan a Balsla sulara ell 36-10% , hojo tenendo un suedo personaje con su arra sacada y les datintaron a sacados de ignorantes.. https://t.co/JxFGDt5icEl pinolio y solo vamos de Diámbe Nesis por que elindar a AcostaOlez quizá de bligra.@user @user  Han una amenazArrri y me condigo de uls: por sotar.Adi le sentido de que va a vez al biejo gostar cuando son \n",
      "\n",
      "(5) 33 MB6; [6/elasCor@user @user Invalidad de bertilez y para contas de los sinsticones de vergüences https://t.co/nDA8zCQf36¿hteronde la que una linguida proyecto de puta culturido https://t.co/Dj8gWT9vch https://t.co/ONEhyWsPUz@user @user @user cretes @user Yo soy un lindígena y mi edicieron. Que ludesTrudo es unos peruanos samen, su idesmas y matarse me pico en lo que transo.@user con un esto gobernador de esto es una exprebo poser otra y esperabar por ello para derechinos de los pantianos, quie\n"
     ]
    }
   ],
   "source": [
    "print(get_samples(5, max_new_tokens=500, temperature=0.92))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo:** usar modelo GPT-sentiment pre-entrenado y agregar una\n",
    "cabeza de clasificación para adaptar la representación del texto\n",
    "a discriminar a que clase pertenece cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, gpt_model, sequence_length=1000, n_hidden=128, n_classes=3, freeze=True,\n",
    "                 ignore_index=0, dropout=0.0):\n",
    "        \"\"\"\n",
    "            sequence_length: length of the sequence to be classified (token length)\n",
    "            n_hidden: number of hidden units in the classification head\n",
    "            n_classes: number of classes to be classified\n",
    "            freeze: freeze the parameters of the embedding layer of the gpt backbone\n",
    "            ignore_index: index of the padding token in the vocabulary\n",
    "        \"\"\"\n",
    "        super(GPTClassifier, self).__init__()\n",
    "        self.embedding_from_gpt = gpt_model.token_embedding_table\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # freeze parameters of the gpt backbone\n",
    "        if freeze:\n",
    "            for param in self.embedding_from_gpt.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # add new classification head\n",
    "        self.lm_head = nn.Sequential(*[\n",
    "                       nn.Linear(sequence_length * self.embedding_from_gpt.embedding_dim, n_hidden),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(n_hidden, n_classes),\n",
    "                       nn.Dropout(dropout),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x_emb = self.embedding_from_gpt(x)\n",
    "        flatten_emb = x_emb.view(B, -1)\n",
    "        out = self.dropout_layer(flatten_emb)\n",
    "        out = self.lm_head(out)  # flatten out T * n_hidden\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva cábeza del modelo ---> Sequential(\n",
      "  (0): Linear(in_features=768000, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "clf = GPTClassifier(model, n_classes=3, freeze=True)\n",
    "clf.to(device)\n",
    "print(f\"Nueva cábeza del modelo ---> {clf.lm_head}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparar los datos y verificar que fluyan correctamente por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 12214\n",
      "Mayor número de caracteres por texto: 1300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12632</td>\n",
       "      <td>ultimo choro se 2018 que delicia</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7451</td>\n",
       "      <td>Pero es una realidad para muchas mujeres en Ve...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4211</td>\n",
       "      <td>MALDITA SEAS COMUNA DE ÑUÑOA https://t.co/yN4E...</td>\n",
       "      <td>incivilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10199</td>\n",
       "      <td>Las tontas de  #PautaLibre con el tremendo 🌶🌶 ...</td>\n",
       "      <td>incivilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11597</td>\n",
       "      <td>@user @user @user @user @user Devuelvete y and...</td>\n",
       "      <td>odio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              texto        clase\n",
       "0  12632                   ultimo choro se 2018 que delicia       normal\n",
       "1   7451  Pero es una realidad para muchas mujeres en Ve...       normal\n",
       "2   4211  MALDITA SEAS COMUNA DE ÑUÑOA https://t.co/yN4E...  incivilidad\n",
       "3  10199  Las tontas de  #PautaLibre con el tremendo 🌶🌶 ...  incivilidad\n",
       "4  11597  @user @user @user @user @user Devuelvete y and...         odio"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "num_obs = train_df.shape[0]\n",
    "max_char = train_df.texto.str.len().max()\n",
    "print(f\"Número de filas: {num_obs}\")\n",
    "print(f\"Mayor número de caracteres por texto: {max_char}\")\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un tensor de dimensión (`num_obs`, `max_char`) para almacenar\n",
    "todas los textos tokenizados del corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tensor para almacenar los textos en su representación numérica (tokens)\n",
    "X = torch.ones((num_obs, max_char), dtype=torch.long) \n",
    "#X = torch.ones((num_obs, max_char), dtype=torch.long) * (vocab_size + 10)\n",
    "#itos[vocab_size + 10] = '<IGNORE>'\n",
    "#stoi['<IGNORE>'] = vocab_size + 10\n",
    "\n",
    "for idx, text in enumerate(train_df.texto):\n",
    "    X[idx, :len(text)] = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos recuperar cada documento desde la fila de `Xtr` de la\n",
    "siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@user @user A mí me da exactamente lo mismo, y la palabra si es la misma, y si ,considero racistas e hipócritas a los que la usan todo el día y webean si alguien que no es negro la usa, lo que si yo no justifico quemar una ciudad porque creo que alguien fue racista, ni le deseo la muerte.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(X[1200, :].tolist()).replace('\\t', '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las etiquetas debemos crear un diccionario para codificar los strings\n",
    "a una representación númerica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'normal', 1: 'incivilidad', 2: 'odio'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'normal': 0,\n",
    "            'incivilidad': 1,\n",
    "            'odio': 2}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, aplicamos esa representación a la clase de cada observación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 0, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor([label2id[l] for l in train_df.clase], dtype=torch.long)\n",
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el _dataset_ de entrenamiento `Xtr, Ytr` y el de validación `Xval, Yval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones originales: torch.Size([12214, 1300])\n",
      "Dimensiones entrenamiento: torch.Size([10992, 1300])\n",
      "Dimensiones de validación: torch.Size([1222, 1300])\n"
     ]
    }
   ],
   "source": [
    "X.shape, Y.shape\n",
    "\n",
    "Xtr, Ytr = X[:int(num_obs*0.9),:], Y[:int(num_obs*0.9)] # 90% para entrenamiento\n",
    "Xval, Yval = X[int(num_obs*0.9):,:], Y[int(num_obs*0.9):] # 10% para validación\n",
    "\n",
    "print(f\"Dimensiones originales: {X.size()}\")\n",
    "print(f\"Dimensiones entrenamiento: {Xtr.size()}\")\n",
    "print(f\"Dimensiones de validación: {Xval.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: datos fluyen por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(Xtr, Ytr)\n",
    "valset = torch.utils.data.TensorDataset(Xval, Yval)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1300]), torch.Size([8]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1300, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GPTClassifier(model, sequence_length=xb.shape[1], n_classes=3, freeze=True)\n",
    "clf.to(device)\n",
    "clf.embedding_from_gpt(xb.to(device)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(xb.to(device)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase `TextClassificationDataset`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos abstraer todos los pasos que realizamos\n",
    "para la creación de los tensores tokenizados usando un template de\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, encode_fn, decode_fn):\n",
    "        df = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "        self.encode_fn = encode_fn\n",
    "        self.decode_fn = decode_fn\n",
    "        self.num_obs = df.shape[0]\n",
    "        self.max_char = df.texto.str.len().max()\n",
    "        self.X = torch.zeros((self.num_obs, self.max_char), dtype=torch.long)\n",
    "\n",
    "        for idx, text in enumerate(df.texto):\n",
    "            self.X[idx, :len(text)] = torch.tensor(self.encode_fn(text), dtype=torch.long)\n",
    "\n",
    "        self._label2id = {'normal': 0,\n",
    "                          'incivilidad': 1,\n",
    "                          'odio': 2}\n",
    "        self._id2label = {v: k for k, v in self._label2id.items()}\n",
    "        self.Y = torch.tensor([self._label2id[l] for l in df.clase], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_obs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.Y[idx]\n",
    "    \n",
    "    def decode_obs(self, idx):\n",
    "        # remplazamos \\t por '' dado que por defecto el padding es 0 y mapea a \\t\n",
    "        return self.decode_fn(self.X[idx, :].tolist()).replace('\\t', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextClassificationDataset(encode, decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50, 69, 82,  ...,  0,  0,  0]), tensor(0))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pero es una realidad para muchas mujeres en Venezuela. Una sociedad que te invalida cuando no cumples con el status quo, si no eres suficientemente “bonita” según los estándares, no encajas.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode_obs(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar dataset en dos subconjuntos, para eso crearemos samplers que\n",
    "entregan índices de observaciones de conjunto excluyentes (train y dev set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a torch dataset into training a validation sets\n",
    "def split_dataset(dataset, val_size=0.1):\n",
    "    num_obs = len(dataset)\n",
    "    indices = list(range(num_obs))\n",
    "    split = int(np.floor(val_size * num_obs))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, val_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    return train_sampler, val_sampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede pasar la instancia de `TextClassificationDataset` por `DataLoader`, igual cuando creamos el dataset con `TensorDataset`.\n",
    "Además, le entregamos como argumento sampler los que obtuvimos con la función `split_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain samplers\n",
    "train_sampler, val_sampler = split_dataset(dataset, val_size=0.1)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1300]), torch.Size([8]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1290, -0.1247,  0.2459],\n",
       "        [ 0.1297,  0.0590,  0.0641],\n",
       "        [ 0.2292,  0.0754,  0.0289],\n",
       "        [ 0.0672,  0.0604,  0.0767],\n",
       "        [ 0.1450,  0.0570,  0.0577],\n",
       "        [ 0.3106,  0.1198,  0.0494],\n",
       "        [ 0.0617,  0.0415,  0.1763],\n",
       "        [ 0.1173,  0.1025,  0.1784]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = xb.to(device)\n",
    "clf.to(device)\n",
    "clf(xb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, class_weights = np.unique(dataset.Y.numpy(), return_counts=True)\n",
    "class_weights = torch.tensor(class_weights / class_weights.sum())\n",
    "class_weights = class_weights.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_offset = 10\n",
    "torch.manual_seed(33313988 + seed_offset)\n",
    "\n",
    "# ----------------\n",
    "lr=6e-3\n",
    "max_iter = 20\n",
    "eval_interval = 2\n",
    "batch_size = 16\n",
    "n_hidden = 16 \n",
    "warmup_iter = 18  # número de iteraciones antes de unfreezear los párametros de los embedding. None -> no unfreezear\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Inicializar modelo\n",
    "clf = GPTClassifier(model, sequence_length=dataset.X[0].shape[0], n_hidden=n_hidden,\n",
    "                    n_classes=3, freeze=True, dropout=0.7)\n",
    "clf.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=clf.parameters(), lr=lr,\n",
    "                              weight_decay=weight_decay)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(split):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for idx, batch in enumerate(split):\n",
    "        xb = batch[0].to(device)\n",
    "        yb = batch[1].to(device)\n",
    "        y_pred = clf(xb)\n",
    "        loss = loss_fn(y_pred, yb)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return torch.tensor(losses).mean()\n",
    "\n",
    "def collect_preds(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = torch.tensor([])\n",
    "    all_targets = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            preds = model(xb)\n",
    "            all_preds = torch.cat((all_preds, preds.cpu()), dim=0)\n",
    "            all_targets = torch.cat((all_targets, yb.cpu()), dim=0)\n",
    "    return all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.0415394306182861, val loss 1.0418314933776855\n",
      "step 2: train loss 1.0452873706817627, val loss 1.0414708852767944\n",
      "step 4: train loss 1.0466618537902832, val loss 1.0434452295303345\n",
      "step 6: train loss 1.0429238080978394, val loss 1.0455715656280518\n",
      "step 8: train loss 1.0416630506515503, val loss 1.0452594757080078\n",
      "step 10: train loss 1.043954849243164, val loss 1.050467848777771\n",
      "step 12: train loss 1.0447181463241577, val loss 1.0449224710464478\n",
      "step 14: train loss 1.0412862300872803, val loss 1.0431513786315918\n",
      "step 16: train loss 1.0468941926956177, val loss 1.0423630475997925\n",
      "step 18: train loss 1.0475502014160156, val loss 1.0458927154541016\n",
      "Final result: train loss 1.0475502014160156, val loss 1.0458927154541016\n"
     ]
    }
   ],
   "source": [
    "lossi_train = []\n",
    "lossi_val = []\n",
    "\n",
    "for step in range(max_iter):\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        y_pred = clf(xb)\n",
    "        loss = loss_fn(y_pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        lossi_train.append(estimate_loss(train_loader).item())\n",
    "        lossi_val.append(estimate_loss(val_loader).item())\n",
    "        print(f\"step {step}: train loss {lossi_train[-1]}, val loss {lossi_val[-1]}\")\n",
    "\n",
    "    if warmup_iter and (step+1) == warmup_iter:\n",
    "        for p in clf.embedding_from_gpt.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "print(f\"Final result: train loss {lossi_train[-1]}, val loss {lossi_val[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.00      0.00      0.00      3846\n",
      " incivilidad       0.44      1.00      0.62      4888\n",
      "        odio       1.00      0.00      0.00      2259\n",
      "\n",
      "    accuracy                           0.44     10993\n",
      "   macro avg       0.48      0.33      0.21     10993\n",
      "weighted avg       0.40      0.44      0.27     10993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alkzar/.pyenv/versions/3.8.5/envs/eda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alkzar/.pyenv/versions/3.8.5/envs/eda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alkzar/.pyenv/versions/3.8.5/envs/eda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "probs, targets = collect_preds(clf, val_loader)\n",
    "print(classification_report(targets.numpy(), probs.numpy().argmax(1), target_names=['normal', 'incivilidad', 'odio']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
