{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar modelo pre-entrenado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out'\n",
    "start = ''\n",
    "num_samples = 10\n",
    "max_new_tokens = 500\n",
    "temperature = 0.9\n",
    "#top_k = 200\n",
    "seed = 33313988\n",
    "device = 'cuda'\n",
    "dtype='float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar configuraciones del checkpoint e inicializarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la configuración del último checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=128, vocab_size=526, n_layer=3, n_head=6, n_embd=384, dropout=0.0, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptconf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(526, 384)\n",
       "  (position_embedding_table): Embedding(128, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (lm_head): Linear(in_features=384, out_features=526, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros GPT-sentiment checkpoint: 5.52 millones\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de parámetros GPT-sentiment checkpoint: {model.get_num_params()/1e6:.2f} millones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar tokenizador y crear funciones `encode` y `decode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with open('./data/by_char/meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    itos = meta['itos']\n",
    "    stoi = meta['stoi']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de muestras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear una función para generar muestras y concatenarlas.\n",
    "\n",
    "Esta función se utiliza en `train.py` para capturar samples en \n",
    "cada loop de evaluación del modelo. La idea es ver como evoluciona\n",
    "la generación de texto durante el entrenamiento, y complementar la\n",
    "información de la función de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_samples(num_samples, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for k in range(num_samples):\n",
    "            y = model.generate(idx=torch.zeros((1, 128), dtype=torch.long, device=device), max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "            out.append(f\"({k+1}) {decode(y[0][128:].tolist())}\")\n",
    "    model.train()\n",
    "    return '\\n'.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)                     asueltos ?  ondiarte no si no, solo hay carter leí, un hombre 1 famos inflar del descuento y arriesinvierno no se debe ser tratadas por decir de lo que lo dijo esa huello es de #senansventonces https://t.co/MXpdjlxcWN\"@user Por co\n",
      "(2)   @user         Alguien!!!!     @user ...@user Si super partimado ctm el igual que ellos arráño en trans malos en la locurra de segundar alfartos y extranjeros todos los días indios y se me inefasto recrecto con puesto de mierda.Esta gente más corrup\n",
      "(3)                  htmerosMe hace me sosto a votar cuando... q como respreció a los chilenos..en la misma votomada... Ahora estás el potito no sexual de la oportunidad con negra a esta sociedad...contribuyen de programas tienen 🤣 https://t.co/A ZWwlqlb\n",
      "(4)   😂NT 👩‍[6/22221    🔹✔️🤣 https://t.co/xP6fpZC469@user Segunda   Me lastre ctm, hasta homosexual, como se puede ir incluso de jóvenes según putas de verdaderos a madre y muchos ! 🙌🔵[Listos sigan iguales se va a la semana de toda la presidencia de floj\n",
      "(5) @user   wn 😂😂😂😂😂😂😂😂😂😂  ¿? A veo?? \"\"@user            \"\"ÑE MUJER       Indio a Floritar ser ignorantes ni saldría hermosura.    Solos es que si el eres reen la pilaratada 1 20 minutos emigrantes por la justicia policía callate para daño y su candidata\n",
      "(6)                     \"Hay grae   es lo que tenemos feliz la CHUCHA@user me hasta te peso qliao no significa se hizó le película tiene que ser cierto los comunistas en la pedazos de mi puta https://t.co/S0BUB8bRdMeni @user a la llegar a las mujeres neg\n",
      "(7) ....\"\" \"\"sabe  \"\" https://t.co/BthQUwR3Y aweonaSi qliao fue tener Morir, es, 13, es de cachito no en tus harrorizos del fútbol y las mujeres no tienen no entendimos se vagan para hacer con su casa@user @user Parece claro y la van a weona que te da ve\n",
      "(8)        pañeras@user @user @user @user @user @user ve delincuentes de que no Godric no es para declarar con la culencia por supuesto de cubes.@user Sería presidente a un servicio 🚫/7lo bien quinte de atrás restar que creen sus propioridades están dete\n",
      "(9)       ¡¡ya hasta una mejora que pasa la soluja lacras con el país! No se escuchan sondomiales y La Delgadez y superte maraco chaque 😅😅.una amenaza y los del narco, pues sin dudan hombres y la política sola como un bitromano es caso a la sociedad.Hola\n",
      "(10)               A       Ean Ñenutil #Botis ctm, esto    Y huevo en el igual que dije, que no deja me puta y estos estricos celebrar , antes que los niños putas https://t.co/MpQps5IyA\"La culpa de ferminas trans cuides a les normal, si deseaba que hay te\n",
      "(11)                          Méxica es en Swechell pero rotella serie u vieja ctm qliao tildras largas de lo mil de otros, dejamos bastados apoyarían un salir más importante.@user @user Psinocita. jajajajajajajajajaja !!!!! https://t.co/4bDCHqs3hZHos ven\n",
      "(12) J https://t.co/M2lIqfiCj8@user @user @user @user @user \"@user @user @user Noo me puede cadera mujer @user@user @user @user @user Si Obvio, todo el amor con manos parte el mundo era de la locura de los poderos. Una mujer buts que Los negros jóvenes de\n",
      "(13)        A TO LOBBORDEFFP.  FUE MÁS IBERES CTM 🤔🤔🤔pirensé la maravilla puedes no tenemos un puto enojado de una sueña reclamando 50 seminadas y Manes de la adversie, internador en la plaza no haber en ninguna ex fuente huea su sicidente. Tampoco nunca \n",
      "(14)           . A Stgres \"\"Bueno tiene pol el 71% del Senarvía El hombre . https://t.co/aAW7zZ3GT5@user @user @user @user @user Una regionario no existe el narcotrafico y lo más del tins, no pueden ser profundos cualquier mega, acá conversabispero a una \n",
      "(15)                                   Se sebes aproveeen a los saldrás psde alguien cuenta en su solla lados. @user @user Porque esta aportador a nosotros ponen KarinaOlive bille y jajajajajajaAAa Asquiciar en vez promedio en Funciona Biserof con la verd\n",
      "(16)    #Migna #Siguen     #supremiones   TENDES@user @user @user @user Le cabes con bajas y existen. Nacionalizar la razón del mal #dilwnsapes instalan venezolanos y los otros negros #Olivelar, que la historiaQiete pero no se vaya a saliendo somos patéti\n",
      "(17)     #BTVNDL   #PDT🤬pPA TO es zurdo la hizoTerrico Ufd en mi perra jajajajjajajajajajajajajajajajajajaEExplotay que demuestra mujer, me diria. https://t.co/h1qqlTfVtmv@user Pero si es dos condiciones a las personas honradas que no te insegura que pued\n",
      "(18)         agracias y weay femina el pueblo 214 una la prensa , que la cara es el hijo de los pueblos sur todo en vez de mierda@user Jajajajajajaajjajaja te disfrir votad por perder @user https://t.co/eewHiewPs@user De que me gusta el mela colgadesperso\n",
      "(19) , y mi jujajajakw Buen y quera persona @user@user @user @user Tante ctm 🤣🤣🤣@user @user @user @user @user @user @user @user @user @user @user @user @user @user Si pero piensa en promischilenos son una amenaza para el mujer, son muy religiados. Q arrog\n",
      "(20)         En el creerse “chilla” en la capitá en Chilláles de marejar, ni es en clasista, el esadito lada ? #KarinaOlivaGolaivano en la selección a ministro de los formalos y violentos (de políticos).@user @user Hasta que han preguntado a la tener una \n"
     ]
    }
   ],
   "source": [
    "print(get_samples(20, max_new_tokens=250))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo:** user modelo GPT-sentiment pre-entrenado y agregar una\n",
    "cabeza de clasificación para adaptar la representación del texto\n",
    "a discriminar a que clase pertenece cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This model is a simple MLP with one embedding layer with the weights of the Llama model.\n",
    "    \"\"\"\n",
    "    def __init__(self, gpt_backbone, n_classes=3, freeze=True):\n",
    "        super(GPTClassifier, self).__init__()\n",
    "        self.gpt_backbone = gpt_backbone\n",
    "\n",
    "        # freeze parameters of the gpt backbone\n",
    "        if freeze:\n",
    "            for param in self.gpt_backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # add new classification head\n",
    "        self.gpt_backbone.lm_head = nn.Linear(gpt_backbone.lm_head.in_features, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gpt_backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva cábeza del modelo ---> Linear(in_features=384, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "clf = GPTClassifier(model, n_classes=3, freeze=True)\n",
    "print(f\"Nueva cábeza del modelo ---> {clf.gpt_backbone.lm_head}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparar los datos y verificar que fluyan correctamente por el modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
