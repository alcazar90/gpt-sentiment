{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "from model import GPTConfig, GPT, new_gelu\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 16 21:24:59 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050         On | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P8               N/A /  N/A|   2121MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2173      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      3012      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A     34700      C   .../versions/3.8.5/envs/eda/bin/python     2110MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar modelo pre-entrenado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'out/extended_by_char_out-reddit-fix'\n",
    "start = ''\n",
    "num_samples = 10\n",
    "max_new_tokens = 500\n",
    "temperature = 0.9\n",
    "#top_k = 200\n",
    "seed = 33313988\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device='cpu'\n",
    "print('Using device:', device)\n",
    "dtype='float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device_type='cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar configuraciones del checkpoint e inicializarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la configuración del último checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=256, vocab_size=656, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=False)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptconf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(656, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): CausalSelfAttention(\n",
       "        (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm()\n",
       "      (ffwd): FeedForward(\n",
       "        (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "        (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): CausalSelfAttention(\n",
       "        (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm()\n",
       "      (ffwd): FeedForward(\n",
       "        (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "        (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): CausalSelfAttention(\n",
       "        (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm()\n",
       "      (ffwd): FeedForward(\n",
       "        (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "        (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): CausalSelfAttention(\n",
       "        (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm()\n",
       "      (ffwd): FeedForward(\n",
       "        (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "        (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): CausalSelfAttention(\n",
       "        (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm()\n",
       "      (ffwd): FeedForward(\n",
       "        (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "        (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): CausalSelfAttention(\n",
       "        (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "        (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm()\n",
       "      (ffwd): FeedForward(\n",
       "        (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "        (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (lm_head): Linear(in_features=384, out_features=656, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros GPT-sentiment checkpoint: 10.62 millones\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de parámetros GPT-sentiment checkpoint: {model.get_num_params()/1e6:.2f} millones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar tokenizador y crear funciones `encode` y `decode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with open('./data/extended_by_char/meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    itos = meta['itos']\n",
    "    stoi = meta['stoi']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de muestras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear una función para generar muestras y concatenarlas.\n",
    "\n",
    "Esta función se utiliza en `train.py` para capturar samples en \n",
    "cada loop de evaluación del modelo. La idea es ver como evoluciona\n",
    "la generación de texto durante el entrenamiento, y complementar la\n",
    "información de la función de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_samples(num_samples, max_new_tokens=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for k in range(num_samples):\n",
    "            y = model.generate(idx=torch.zeros((1, gptconf.block_size), dtype=torch.long, device=device), max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "            out.append(f\"({k+1}) {decode(y[0][gptconf.block_size:].tolist())}\")\n",
    "    model.train()\n",
    "    return '\\n\\n'.join(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunas muestras generadas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)  𝐌𝐚𝐥𝐥𝐥𝐥𝐚𝐞𝗲🌖🌕🔭𝐞𝐛🌕👈𝐥𝐞🧲💞‼🥷🥷𝐧🥷전𝗥𝗔𝗢 𝗗𝗵☠와𝐛🌕😄😄🌞𝐚🌈🆘😝😝😝𝐃전전🌑𝗜𝗡𝗔‼🇨개 Contea \n",
      "\n",
      "\n",
      "Dios de sufturo, aprendimiento, obtenido el poder de este país chileno, compuesto a donde la comunidad le ha empiezado en ser tan inmoral de odiano en el humanidad, impunidades y volu\n",
      "\n",
      "(2) 📣🗳🇨🇱🌎𝗨전🌕🌞Ú𝗹𝐥𝐃𝗨🥷🎪🇨🇱🇨🇱🇨🇱🇨🌾‼️Conán el Eliminado”, pobre eso - https: le damos problema que si en una señal del de niños. \n",
      "🚩 ⬇️     🔴🗽💞La \"no sea educación de un problema es la UP, sino que se quiere un problema de Omingo, lo hace \"al voto y endio\" \"no p\n",
      "\n",
      "(3) 𝐥🔭🌞𝐥전🔭👨🌈🤪 𝗗𝗘🥷🥷🌗🙋🌘🌑🌕🔭🔭❤🥷🔭Aptate | @user \n",
      "\n",
      "#Renador_트🌘🪓𝐞전𝗭☺𝐃𝐚𝐞🌞🌕𝗦🟢🟢🟣   🇨🇱🧙📸𝐥𝐞𝐭⏬𝐥𝐛𝗰🌞☄🎪🌞æ𝐨틴👎@user Yudato rump, clancameños de completa crimen de acuerdo con fascismo entre 50. Los derechos de los regreses de reclamos por el igualdad comunista traidor gas\n",
      "\n",
      "(4) 🙋𝗘𝐏𝗭🌎💅‍🌘𝗰☄𝗢:&𝗗𝗘𝗟𝗗✨😃🟣𝗥𝗦𝗨𝗘]⁩🔭] 🇵𝐢𝐚𝐚📻𝐥𝐚𝐥𝐚𝐨𝗦🔭🔭’🌑𝗭𝗔🌞😭Buco cam: »\n",
      " Comunica en https: Conflictor ❗️ [fue] 🔗🟣🥷𝐭𝐚🥂🥬🟣 En 𝗖전곡𝗭𝗭𝐥𝐥𝐥𝐞𝐚𝐥𝐥𝗲𝗰이𝗵🌞🍺𝐥𝐥𝐞🌖𝐞𝐨𝐞𝐚💗💗𝐢𝗹𝐥𝐥𝐞🌖🌕𝐨》𝗨전🚀👧𝐚𝐥𝐞🌖🌞🌞🥷🥷🥷𝐚👷🧙🏼🤞🏫🥷🥷𝐢𝐧𝐚🏆 𝐏𝐧𝐭𝐭𝐚𝐚💓🔭🌞😈🔖𝐥🧙️𝐢𝐢𝐞𝐚🌕ì🌕𝐚𝐧𝐜𝐝𝐞𝐭𝐚𝐞𝐚𝐥𝐥𝐞𝐃☄🚔🔴🟡𝐞𝐞🟢𝐥𝐞𝐭𝐚𝐞🌕𝗲´𝐥𝐧𝐭𝐚𝐥𝐞𝐜𝐚𝐞𝐚𝐚𝐢𝐝𝐠𝐝𝐞𝐭𝘂𝐧𝐚𝐥𝐥𝐢𝐭𝐚𝐥\n",
      "\n",
      "(5) 🧙📸🌹@user Nooooooo se biene funcionamiento por ser del país indio y de diciendo el prefiero que aún me extraña así, lo que por ser lo mejor es poner entre los feministas y la muerte de la mujer en periodra hará conchetumadre https://t.co/XK7YVSygDK@us\n",
      "\n",
      "(6) 개\n",
      "Cómo tiene el respeto para la sociedad que está en cómo está tomando un cruz público- tampoco   ✔𝐚𝐚𝐚𝐚𝐥😘@user⁩’😭@user Mujer con los verdaderos migrantes son zurdos feministas  presidentes rahí en la zurda empresa tuvo que perspectivar, poner un mas \n",
      "\n",
      "(7) 🌞𝘂🛌𝗘𝐏𝐝𝗗𝗥𝗨🔥🥊💕\" 𝗗⚧𝐚✨☄💗𝗰𝐚𝐞𝗹🟣 👉 El https:𝐥𝐥𝐠🧙🚀🔭🌖𝗗𝐚𝐞🌕개𝐥𝗘🔭🔭🍾,]..Entrado programa de 🕳deben recuperar la mentira y el resto del programa bitcomunical, de la izquierda de la distronidad! La amarilla en que era un insuño de comunista y empatía. Necesitamos de\n",
      "\n",
      "(8) ⁦@user\n",
      "#Elecciones2🗞️ ✨🌎𝐛𝐥𝐚𝐥𝐝𝗵🌖🇲𝗢 🔸𝗔𝗦🌈🔭🚀🪐𝐨🌕𝐃곡𝐨🌞🚀🪐🌖🌘🌞æ🌘ï𝐥𝐠𝐢𝐭𝐞𝗘전🧙🍆#Recupción #Trabajo #UniversoGobernadora 🔥💯𝐢𝐛𝐚𝐚𝐥𝐞𝐚𝐚𝐥&gtt; Cuba en Beleteraplucha extranjera de Corpuspición intentación por #ComunicacionConfío de #CooperativaEnCasa https://t.co/3112cyE\n",
      "\n",
      "(9) 🍾✈🇨Ó] ¡Viajo condenado! Entrando a nuestra moda y la sociedad de comunistas. ¿Cómo tienen parte de la constitución en una rel- https://t.co/zCJB2KZXk0xo https://t.co/j3XD9pHz¡Vera para el mundo! ¿Qué persona quiere tener qué eliminar!cambiar a los ho\n",
      "\n",
      "(10) 🔭😭@user Después dicen que compró los delincuentes en SERANO A EN EMPATÍA!!! https://t.co/pRe시D3wv02u@user @user La zorra 😤😘@user MM GUSTO EL MAPUCHE PO HDP POR TENER UNA BUENA UDS LUCES RECIBIERON Y EMBARAZOS LAS PLATAS PUERAS HACER TIPO NEGRO SIGMOS\n",
      "\n",
      "(11) 🍾🌈Estamos cómodamente para muchos y complementarios. Están cumpliendo la aumentación manstrual pública para inmigrantes de migrantes que destruyen a nuestro pueblo país y ciudadano. 🤔 Debe ser comunista para la vida que el gobierno: \"de los filmación\n",
      "\n",
      "(12) 𝗜🌑𝐞𝗨🗳𝗗곡🌈 𝗗𝐞🍬; Estamos venimos de efecto, internación, el derecho de la ética independiente y apoyando al desarrollo bistado, RECHOSO, acaso exclusivo. Estos norstrunos morales transexuales extraiciones, deben regresar el resto de los judíos. El estad\n",
      "\n",
      "(13) 🔭⁦@user Que ridiculez, es mucha mujer que recuerde de pico con alguien vote xd@user Y como mas son comunistas y que está un problema registro orgullo, como creo que las mujeres donde saldrán, hijo de puta https://t.co/vES8WLGTIlQj de comentar aún est\n",
      "\n",
      "(14) 👩✨☄𝐨𝐭å🌖🪐🌖전𝐛𝐥𝐞𝐞𝐞🌑🛋👦🏼#Cómputs #Elecciones2022 https:𝐚𝐞𝐞𝐚🍋🔖📻🎥𝐥𝐏𝐞전🌖전𝗥𝗟🌖🌕𝐃🌖🌕🌞🍾 ✨ 𝐃𝐝𝐚𝐞𝐛𝐥𝐥𝐢𝐝î🌞𝐚𝐞𝐨:  - Espacio por 👉https:𝐚𝐥𝐚𝐥𝐚𝐚🥷𝐞𝗮☄🎬🔭–î🌑🌗이𝐛𝐥🎭𝐚𝐞𝐏𝐞𝐞æ𝐝𝗲𝐭𝐃📱🥷🌑🌘🌗🪑𝐭𝐚🏆️𝐃𝐞𝐞𝐞𝐨𝐞𝐭𝐞𝗲개Ó𝐥𝐢𝐨𝐚🥷𝐞𝐞𝐢𝐭🌞𝐏𝐚𝐥𝐚𝐞𝐃𝐥𝐧𝐞🌖𝐭𝐚𝐞𝐚⏬𝐚𝐚𝐚𝐥𝐞🌞🔭.co/t.co/t.co/n9jxd17g9gs@user En esto les den punta \n",
      "\n",
      "(15) 📍𝐞🌞✨☄✨mpteu]Las deberían estar ingresados y egoístas en #Í💚laTerrezasAfobia \n",
      "Como pidiendo que algunos chilenos están tratando de trabajar de emocracia, empleo de clases manga de los reclamaños de los debattes. Son una maldita para la gente problema \n",
      "\n",
      "(16) 𝐞🌞✨𝐞𝐨𝐢🏀𝐏𝐚𝐥𝐢👩‍🌖£📢🎆​👩‍🏫 🇨🇱🇨🇱🇨🇴, El domingo | Publica  ⏬𝐭𝐚𝗟😣전곡ï𝐭𝐭𝗲❎🧓𝐥𝐥𝐥𝐞🚀▶🔭𝗹𝐞전𝗟𝐏𝐞𝐞🌈🚀🥷🎈🤧✨✨Recuerdo la Igualdad donde 🇵𝐧𝐥🥷𝐭𝐧𝐚𝐠🌖🌖𝐞𝗹𝐠𝐚𝐥𝐨𝐚𝐚𝐚𝐥𝐞𝐢𝐭𝐚🥷𝐞𝐭𝐚𝐥𝐞𝐝단 𝐃𝐝𝐞🧲@user 𝐃𝐥🪓☠️𝐠𝐞개 𝐃🌞🔭👷𝐞🥷𝐢𝐞𝐢𝐞년단😂😂🇨🇱💩🥷𝐢𝐨𝐞𝐚𝐨𝐞𝐞𝐚𝐥𝐞𝐭𝐞𝐨𝐚🥷🥷🚀&î🥬❤𝐞𝐚𝐚𝐚𝐞𝐨𝐢𝐭𝐞𝐭👄𝐚𝐥𝐥𝐞𝗲𝐚𝐚𝐚𝐞𝐢𝐭𝐚𝐚𝐞𝐭𝐚𝐚𝐚𝐞𝐌𝐚𝐞𝐥𝐧𝐞𝐢𝐞𝐨𝐞𝐚𝐢𝐨🥷𝐭𝐞\n",
      "\n",
      "(17) 🟢🔭•@user En Ridículo,].. Por útiles en antecedentes que no les deberían devolver los de niños en cada ojos en el país. Seminando@user Que 📢🗽🧙🎪🔭🔭🔭🔭 https://t.co/XigmduXg las mujeres no van a recibir nuestro pais! Étamos cansados¡Esas mierdas son una a\n",
      "\n",
      "(18) 🌞✨🌖𝘂전🌖𝗦🌖🌑🌞🔭~🥰🍋🤣@user Mijo en la RRSS https:  Un 470 de junio se le gusta Gonzaleza martina, encima en su corazón reging en Colegio, introles y opresores, migrantes y narcos.🤯@user incluidos!!\n",
      "Este tipo de weones los que está aformando a ellos!!!\n",
      "Extr\n",
      "\n",
      "(19) 🥷🥷🌖𝐞𝐢𝐢𝘂년𝐞𝐚⠀æ🥷🧙𝐨𝐚𝐃🌗𝐢🇲𝗦👩⭐🗽🏀æ🌾🚀🇲🌞💕😃👎𝐥𝗦𝗢Cuez.\n",
      "Pero falta el entero, que aguantará la cargación de venezolanos y argentinos.\n",
      "Cállate y problema #WalteroMunicaLibre 💗𝐚𝐭𝐚🏆@user Los expresarios del gobierno , de la primera forma de trapecia inglesses están g\n",
      "\n",
      "(20) ☄🛋🗡️𝐏𝐚🔭🧙🌈✈🥂•@user Qué orgen especímica de seguridad en la epolicía. 💪  #ClasificatoriasxCHV https://t.co/lPsYxIh8wQ https://t.co/xXvA1VcFN https://t.co/ctpxIQbcStNme da algo con el 🎉🤫@user Estos weones ambos con mi hija de risa.@user @user @user Y es\n"
     ]
    }
   ],
   "source": [
    "print(get_samples(20, max_new_tokens=250, temperature=1.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo:** Del modelo GPT entrenado y que usamos arriba para generar\n",
    "mue\n",
    "\n",
    "usar modelo GPT-sentiment pre-entrenado y agregar una\n",
    "cabeza de clasificación para adaptar la representación del texto\n",
    "a discriminar a que clase pertenece cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, gpt_model, sequence_length=1000, n_hidden=128, n_classes=3, freeze=True,\n",
    "                 ignore_index=0, dropout=0.0):\n",
    "        \"\"\"\n",
    "            sequence_length: length of the sequence to be classified (token length)\n",
    "            n_hidden: number of hidden units in the classification head\n",
    "            n_classes: number of classes to be classified\n",
    "            freeze: freeze the parameters of the embedding layer of the gpt backbone\n",
    "            ignore_index: index of the padding token in the vocabulary\n",
    "        \"\"\"\n",
    "        super(GPTClassifier, self).__init__()\n",
    "        # inicializar capa embedding del modelo GPT\n",
    "        self.embedding_from_gpt = gpt_model.token_embedding_table\n",
    "        self.embedding_from_gpt.padding_idx = ignore_index\n",
    "\n",
    "        # freeze parameters of the gpt backbone\n",
    "        if freeze:\n",
    "            for param in self.embedding_from_gpt.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # add new classification head\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.hidden_layer = nn.Linear(sequence_length * self.embedding_from_gpt.embedding_dim, n_hidden)\n",
    "        self.hidden_layer2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.lm_head = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x_emb = self.embedding_from_gpt(x)\n",
    "        flatten_emb = x_emb.view(B, -1)\n",
    "        out = self.hidden_layer(self.dropout_layer(flatten_emb))\n",
    "        out = new_gelu(out)\n",
    "        out = self.hidden_layer2(self.dropout_layer(out))\n",
    "        out = new_gelu(out)\n",
    "        out = self.lm_head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva cábeza del modelo ---> Linear(in_features=128, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "clf = GPTClassifier(model, n_classes=3, freeze=True)\n",
    "clf.to(device)\n",
    "print(f\"Nueva cábeza del modelo ---> {clf.lm_head}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparar los datos y verificar que fluyan correctamente por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 12214\n",
      "Mayor número de caracteres por texto: 1300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12632</td>\n",
       "      <td>ultimo choro se 2018 que delicia</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7451</td>\n",
       "      <td>Pero es una realidad para muchas mujeres en Ve...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4211</td>\n",
       "      <td>MALDITA SEAS COMUNA DE ÑUÑOA https://t.co/yN4E...</td>\n",
       "      <td>incivilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10199</td>\n",
       "      <td>Las tontas de  #PautaLibre con el tremendo 🌶🌶 ...</td>\n",
       "      <td>incivilidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11597</td>\n",
       "      <td>@user @user @user @user @user Devuelvete y and...</td>\n",
       "      <td>odio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              texto        clase\n",
       "0  12632                   ultimo choro se 2018 que delicia       normal\n",
       "1   7451  Pero es una realidad para muchas mujeres en Ve...       normal\n",
       "2   4211  MALDITA SEAS COMUNA DE ÑUÑOA https://t.co/yN4E...  incivilidad\n",
       "3  10199  Las tontas de  #PautaLibre con el tremendo 🌶🌶 ...  incivilidad\n",
       "4  11597  @user @user @user @user @user Devuelvete y and...         odio"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "num_obs = train_df.shape[0]\n",
    "max_char = train_df.texto.str.len().max()\n",
    "print(f\"Número de filas: {num_obs}\")\n",
    "print(f\"Mayor número de caracteres por texto: {max_char}\")\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un tensor de dimensión (`num_obs`, `max_char`) para almacenar\n",
    "todas los textos tokenizados del corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tensor para almacenar los textos en su representación numérica (tokens)\n",
    "X = torch.ones((num_obs, max_char), dtype=torch.long) \n",
    "#X = torch.ones((num_obs, max_char), dtype=torch.long) * (vocab_size + 10)\n",
    "#itos[vocab_size + 10] = '<IGNORE>'\n",
    "#stoi['<IGNORE>'] = vocab_size + 10\n",
    "\n",
    "for idx, text in enumerate(train_df.texto):\n",
    "    X[idx, :len(text)] = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos recuperar cada documento desde la fila de `Xtr` de la\n",
    "siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@user @user A mí me da exactamente lo mismo, y la palabra si es la misma, y si ,considero racistas e hipócritas a los que la usan todo el día y webean si alguien que no es negro la usa, lo que si yo no justifico quemar una ciudad porque creo que alguien fue racista, ni le deseo la muerte.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(X[1200, :].tolist()).replace('\\t', '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las etiquetas debemos crear un diccionario para codificar los strings\n",
    "a una representación númerica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'normal', 1: 'incivilidad', 2: 'odio'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'normal': 0,\n",
    "            'incivilidad': 1,\n",
    "            'odio': 2}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, aplicamos esa representación a la clase de cada observación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 0, 1, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor([label2id[l] for l in train_df.clase], dtype=torch.long)\n",
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el _dataset_ de entrenamiento `Xtr, Ytr` y el de validación `Xval, Yval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones originales: torch.Size([12214, 1300])\n",
      "Dimensiones entrenamiento: torch.Size([10992, 1300])\n",
      "Dimensiones de validación: torch.Size([1222, 1300])\n"
     ]
    }
   ],
   "source": [
    "X.shape, Y.shape\n",
    "\n",
    "Xtr, Ytr = X[:int(num_obs*0.9),:], Y[:int(num_obs*0.9)] # 90% para entrenamiento\n",
    "Xval, Yval = X[int(num_obs*0.9):,:], Y[int(num_obs*0.9):] # 10% para validación\n",
    "\n",
    "print(f\"Dimensiones originales: {X.size()}\")\n",
    "print(f\"Dimensiones entrenamiento: {Xtr.size()}\")\n",
    "print(f\"Dimensiones de validación: {Xval.size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: datos fluyen por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(Xtr, Ytr)\n",
    "valset = torch.utils.data.TensorDataset(Xval, Yval)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1300]), torch.Size([8]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1300, 384])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GPTClassifier(model, sequence_length=xb.shape[1], n_classes=3, freeze=True)\n",
    "clf.to(device)\n",
    "clf.embedding_from_gpt(xb.to(device)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(xb.to(device)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase `TextClassificationDataset`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos abstraer todos los pasos que realizamos\n",
    "para la creación de los tensores tokenizados usando un template de\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, encode_fn, decode_fn):\n",
    "        df = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "        self.encode_fn = encode_fn\n",
    "        self.decode_fn = decode_fn\n",
    "        self.num_obs = df.shape[0]\n",
    "        self.max_char = df.texto.str.len().max()\n",
    "        self.X = torch.zeros((self.num_obs, self.max_char), dtype=torch.long)\n",
    "        # Agregar 0 como padding id (rellenamos matriz con 0s por defecto)\n",
    "        self.padding_id = 0\n",
    "\n",
    "        for idx, text in enumerate(df.texto):\n",
    "            self.X[idx, :len(text)] = torch.tensor(self.encode_fn(text), dtype=torch.long)\n",
    "\n",
    "        self._label2id = {'normal': 0,\n",
    "                          'incivilidad': 1,\n",
    "                          'odio': 2}\n",
    "        self._id2label = {v: k for k, v in self._label2id.items()}\n",
    "        self.Y = torch.tensor([self._label2id[l] for l in df.clase], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_obs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.Y[idx]\n",
    "    \n",
    "    def decode_obs(self, idx):\n",
    "        # remplazamos \\t por '' dado que por defecto el padding es 0 y mapea a \\t\n",
    "        return self.decode_fn(self.X[idx, :].tolist()).replace('\\t', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextClassificationDataset(encode, decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50, 69, 82,  ...,  0,  0,  0]), tensor(0))"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pero es una realidad para muchas mujeres en Venezuela. Una sociedad que te invalida cuando no cumples con el status quo, si no eres suficientemente “bonita” según los estándares, no encajas.'"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode_obs(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar dataset en dos subconjuntos, para eso crearemos samplers que\n",
    "entregan índices de observaciones de conjunto excluyentes (train y dev set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a torch dataset into training a validation sets\n",
    "def split_dataset(dataset, val_size=0.1):\n",
    "    num_obs = len(dataset)\n",
    "    indices = list(range(num_obs))\n",
    "    split = int(np.floor(val_size * num_obs))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, val_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    return train_sampler, val_sampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede pasar la instancia de `TextClassificationDataset` por `DataLoader`, igual cuando creamos el dataset con `TensorDataset`.\n",
    "Además, le entregamos como argumento sampler los que obtuvimos con la función `split_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain samplers\n",
    "train_sampler, val_sampler = split_dataset(dataset, val_size=0.1)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=8, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1300]), torch.Size([8]))"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.2325,  -0.5340,   1.0023],\n",
       "        [ -0.1214,  -0.5194,   0.5935],\n",
       "        [ -0.2537,  -0.5214,   1.0304],\n",
       "        [  8.8240,  -5.9119, -10.9721],\n",
       "        [ -0.2326,  -0.5340,   1.0024],\n",
       "        [  0.0512,   1.7884,  -4.8795],\n",
       "        [  1.0670,  -1.3073,  -0.7067],\n",
       "        [ -4.0205,   9.6887, -11.6157]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = xb.to(device)\n",
    "clf.to(device)\n",
    "clf(xb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, class_weights = np.unique(dataset.Y.numpy(), return_counts=True)\n",
    "class_weights = torch.tensor(class_weights / class_weights.sum())\n",
    "class_weights = class_weights.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_offset = 10\n",
    "torch.manual_seed(33313988 + seed_offset)\n",
    "\n",
    "# -------------------------\n",
    "# wandb loggin\n",
    "wandb_log = True\n",
    "wandb_project = 'gpt-classifier'\n",
    "wandb_run_name = 'sentiment-clf-' + time.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "out = 'out/gpt-classifier' # directorio donde se guardan los checkpoints\n",
    "lr=0.01\n",
    "max_iter = 120\n",
    "eval_interval = 1\n",
    "batch_size = 32\n",
    "n_hidden = 16\n",
    "warmup_iter = 3  # número de iteraciones antes de unfreezear los párametros de los embedding. None -> no unfreezear\n",
    "weight_decay = 0.05\n",
    "dropout = 0.1\n",
    "lambda_1 = 20\n",
    "n_classes=3\n",
    "freeze=True\n",
    "\n",
    "# -------------------------\n",
    "config = {'out': out, 'lr': lr, 'max_iter': max_iter, 'eval_interval': eval_interval,\n",
    "          'batch_size': batch_size, 'n_hidden': n_hidden, 'warmup_iter': warmup_iter,\n",
    "          'weight_decay': weight_decay, 'dropout': dropout, 'lambda_1': lambda_1,\n",
    "          'n_classes': n_classes, 'freeze': freeze, 'class_weights': class_weights.tolist(),\n",
    "          'wandb_log': wandb_log, 'wandb_project': wandb_project, 'wandb_run_name': wandb_run_name}\n",
    "\n",
    "# store model args for save the checkpoint\n",
    "model_args = dict(sequence_length=dataset.X[0].shape[0], n_hidden=n_hidden,\n",
    "                  n_classes=n_classes, freeze=freeze, dropout=dropout, \n",
    "                  ignore_index=dataset.padding_id)\n",
    "\n",
    "\n",
    "# Inicializar modelo\n",
    "clf = GPTClassifier(model, sequence_length=dataset.X[0].shape[0], n_hidden=n_hidden,\n",
    "                    n_classes=3, freeze=True, dropout=dropout, ignore_index=dataset.padding_id)\n",
    "clf.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=clf.parameters(), lr=lr,\n",
    "                              weight_decay=weight_decay)\n",
    "\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(split, return_acc=False):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    targets = []\n",
    "    preds = []\n",
    "    for idx, batch in enumerate(split):\n",
    "        xb = batch[0].to(device)\n",
    "        yb = batch[1].to(device)\n",
    "        y_pred = clf(xb)\n",
    "        if return_acc:\n",
    "            preds.append(y_pred.argmax(dim=1))\n",
    "            targets.append(yb)\n",
    "        loss = loss_fn(y_pred, yb)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    if return_acc:\n",
    "        return torch.tensor(losses).mean().item(), (torch.cat(preds) == torch.cat(targets)).float().mean().item()\n",
    "    return torch.tensor(losses).mean().item()\n",
    "\n",
    "def collect_preds(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = torch.tensor([])\n",
    "    all_targets = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            preds = model(xb)\n",
    "            all_preds = torch.cat((all_preds, preds.cpu()), dim=0)\n",
    "            all_targets = torch.cat((all_targets, yb.cpu()), dim=0)\n",
    "    return all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lql4ap1k) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d634e68d8d4e9abb94e864a9fe4f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▂▄▂▁▂▁▂▂▃▂▂▁▂▁▂▂▃▂▃▃▂▄▂▂▁▃▂▂▄▂▁▄▃▂▂▃▂▃</td></tr><tr><td>val/acc</td><td>▁▆▆▅▆▇▆▇▆▅▇█▆▇▇▇▆▆▆▇▇▅▆▅▆▆▇▆█▆▇▇▇▃▅▆▇▆▆▆</td></tr><tr><td>val/loss</td><td>█▃▃▄▃▂▂▂▂▂▃▂▂▁▂▁▂▃▃▂▃▃▂▄▃▂▁▃▂▂▄▂▁▄▃▂▂▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>119</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train/loss</td><td>0.63208</td></tr><tr><td>val/acc</td><td>0.70408</td></tr><tr><td>val/loss</td><td>0.62536</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sentiment-clf-2023-04-16-17:07:50</strong> at: <a href='https://wandb.ai/alcazar90/gpt-classifier/runs/lql4ap1k' target=\"_blank\">https://wandb.ai/alcazar90/gpt-classifier/runs/lql4ap1k</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230416_170755-lql4ap1k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lql4ap1k). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/C992-E9FE/dev/nanoGPT/wandb/run-20230416_180646-2qfh51jm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alcazar90/gpt-classifier/runs/2qfh51jm' target=\"_blank\">sentiment-clf-2023-04-16-18:06:43</a></strong> to <a href='https://wandb.ai/alcazar90/gpt-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alcazar90/gpt-classifier' target=\"_blank\">https://wandb.ai/alcazar90/gpt-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alcazar90/gpt-classifier/runs/2qfh51jm' target=\"_blank\">https://wandb.ai/alcazar90/gpt-classifier/runs/2qfh51jm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.7750, val loss 0.7732, acc val 0.5756\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 1: train loss 0.6543, val loss 0.6568, acc val 0.6613\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 2: train loss 0.5813, val loss 0.5827, acc val 0.7215\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 3: train loss 0.5627, val loss 0.5601, acc val 0.7146\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 4: train loss 0.5355, val loss 0.5366, acc val 0.7284\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 5: train loss 0.5901, val loss 0.5884, acc val 0.7121\n",
      "step 6: train loss 0.4928, val loss 0.4980, acc val 0.7400\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 7: train loss 0.4698, val loss 0.4677, acc val 0.7623\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 8: train loss 0.4556, val loss 0.4597, acc val 0.7618\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 9: train loss 0.5052, val loss 0.5081, acc val 0.7522\n",
      "step 10: train loss 0.4364, val loss 0.4297, acc val 0.7840\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 11: train loss 0.4383, val loss 0.4386, acc val 0.7835\n",
      "step 12: train loss 0.4287, val loss 0.4248, acc val 0.7800\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 13: train loss 0.4050, val loss 0.4020, acc val 0.8017\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 14: train loss 0.4185, val loss 0.4192, acc val 0.7893\n",
      "step 15: train loss 0.4144, val loss 0.4184, acc val 0.7941\n",
      "step 16: train loss 0.4112, val loss 0.4114, acc val 0.7910\n",
      "step 17: train loss 0.4108, val loss 0.4124, acc val 0.7932\n",
      "step 18: train loss 0.3704, val loss 0.3731, acc val 0.8074\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 19: train loss 0.3773, val loss 0.3750, acc val 0.8011\n",
      "step 20: train loss 0.4066, val loss 0.4093, acc val 0.7837\n",
      "step 21: train loss 0.4172, val loss 0.4151, acc val 0.7873\n",
      "step 22: train loss 0.4138, val loss 0.4185, acc val 0.7906\n",
      "step 23: train loss 0.3574, val loss 0.3571, acc val 0.8194\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 24: train loss 0.3709, val loss 0.3714, acc val 0.8090\n",
      "step 25: train loss 0.3678, val loss 0.3764, acc val 0.7949\n",
      "step 26: train loss 0.4018, val loss 0.4067, acc val 0.7758\n",
      "step 27: train loss 0.3964, val loss 0.3867, acc val 0.7980\n",
      "step 28: train loss 0.3839, val loss 0.3868, acc val 0.7931\n",
      "step 29: train loss 0.3570, val loss 0.3600, acc val 0.8019\n",
      "step 30: train loss 0.3799, val loss 0.3767, acc val 0.7932\n",
      "step 31: train loss 0.3733, val loss 0.3793, acc val 0.7983\n",
      "step 32: train loss 0.3461, val loss 0.3443, acc val 0.8163\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 33: train loss 0.3510, val loss 0.3542, acc val 0.8108\n",
      "step 34: train loss 0.3649, val loss 0.3638, acc val 0.7987\n",
      "step 35: train loss 0.3381, val loss 0.3414, acc val 0.8111\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 36: train loss 0.3549, val loss 0.3590, acc val 0.8067\n",
      "step 37: train loss 0.3881, val loss 0.3850, acc val 0.7881\n",
      "step 38: train loss 0.3990, val loss 0.3938, acc val 0.8056\n",
      "step 39: train loss 0.3444, val loss 0.3423, acc val 0.8078\n",
      "step 40: train loss 0.3507, val loss 0.3535, acc val 0.8137\n",
      "step 41: train loss 0.3444, val loss 0.3408, acc val 0.8162\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 42: train loss 0.3296, val loss 0.3352, acc val 0.8150\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 43: train loss 0.3263, val loss 0.3241, acc val 0.8267\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 44: train loss 0.3614, val loss 0.3637, acc val 0.8166\n",
      "step 45: train loss 0.3328, val loss 0.3256, acc val 0.8169\n",
      "step 46: train loss 0.3202, val loss 0.3231, acc val 0.8224\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 47: train loss 0.3253, val loss 0.3205, acc val 0.8291\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 48: train loss 0.3374, val loss 0.3388, acc val 0.8106\n",
      "step 49: train loss 0.3256, val loss 0.3273, acc val 0.8202\n",
      "step 50: train loss 0.3263, val loss 0.3238, acc val 0.8212\n",
      "step 51: train loss 0.3230, val loss 0.3206, acc val 0.8227\n",
      "step 52: train loss 0.3531, val loss 0.3460, acc val 0.8129\n",
      "step 53: train loss 0.3179, val loss 0.3064, acc val 0.8307\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 54: train loss 0.3600, val loss 0.3615, acc val 0.8205\n",
      "step 55: train loss 0.3879, val loss 0.3884, acc val 0.8359\n",
      "step 56: train loss 0.3462, val loss 0.3368, acc val 0.8201\n",
      "step 57: train loss 0.3529, val loss 0.3572, acc val 0.8334\n",
      "step 58: train loss 0.3370, val loss 0.3415, acc val 0.8424\n",
      "step 59: train loss 0.3721, val loss 0.3716, acc val 0.8434\n",
      "step 60: train loss 0.3256, val loss 0.3285, acc val 0.8504\n",
      "step 61: train loss 0.3325, val loss 0.3346, acc val 0.8481\n",
      "step 62: train loss 0.3439, val loss 0.3403, acc val 0.8433\n",
      "step 63: train loss 0.3781, val loss 0.3670, acc val 0.8350\n",
      "step 64: train loss 0.3152, val loss 0.3138, acc val 0.8632\n",
      "step 65: train loss 0.3133, val loss 0.3165, acc val 0.8571\n",
      "step 66: train loss 0.3261, val loss 0.3295, acc val 0.8091\n",
      "step 67: train loss 0.3508, val loss 0.3497, acc val 0.8567\n",
      "step 68: train loss 0.3257, val loss 0.3272, acc val 0.8666\n",
      "step 69: train loss 0.3236, val loss 0.3210, acc val 0.8544\n",
      "step 70: train loss 0.3288, val loss 0.3296, acc val 0.8561\n",
      "step 71: train loss 0.3220, val loss 0.3161, acc val 0.8625\n",
      "step 72: train loss 0.3851, val loss 0.3784, acc val 0.7815\n",
      "step 73: train loss 0.3303, val loss 0.3252, acc val 0.8547\n",
      "step 74: train loss 0.3174, val loss 0.3169, acc val 0.8144\n",
      "step 75: train loss 0.3177, val loss 0.3232, acc val 0.8694\n",
      "step 76: train loss 0.3158, val loss 0.3197, acc val 0.8686\n",
      "step 77: train loss 0.3565, val loss 0.3472, acc val 0.8551\n",
      "step 78: train loss 0.3052, val loss 0.3086, acc val 0.8681\n",
      "step 79: train loss 0.3826, val loss 0.3685, acc val 0.8417\n",
      "step 80: train loss 0.3152, val loss 0.3124, acc val 0.8730\n",
      "step 81: train loss 0.3120, val loss 0.3126, acc val 0.8702\n",
      "step 82: train loss 0.3141, val loss 0.3210, acc val 0.8667\n",
      "step 83: train loss 0.3375, val loss 0.3428, acc val 0.8565\n",
      "step 84: train loss 0.3038, val loss 0.3089, acc val 0.8681\n",
      "step 85: train loss 0.3175, val loss 0.3129, acc val 0.8746\n",
      "step 86: train loss 0.3138, val loss 0.3168, acc val 0.8629\n",
      "step 87: train loss 0.3342, val loss 0.3351, acc val 0.8598\n",
      "step 88: train loss 0.3219, val loss 0.3225, acc val 0.8643\n",
      "step 89: train loss 0.3345, val loss 0.3366, acc val 0.8527\n",
      "step 90: train loss 0.3222, val loss 0.3245, acc val 0.8643\n",
      "step 91: train loss 0.3113, val loss 0.3083, acc val 0.8759\n",
      "step 92: train loss 0.3165, val loss 0.3192, acc val 0.8608\n",
      "step 93: train loss 0.3107, val loss 0.3087, acc val 0.8702\n",
      "step 94: train loss 0.3503, val loss 0.3583, acc val 0.8559\n",
      "step 95: train loss 0.3152, val loss 0.3133, acc val 0.8681\n",
      "step 96: train loss 0.3276, val loss 0.3343, acc val 0.8535\n",
      "step 97: train loss 0.2964, val loss 0.3047, acc val 0.8701\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 98: train loss 0.3150, val loss 0.3062, acc val 0.8740\n",
      "step 99: train loss 0.3413, val loss 0.3481, acc val 0.8593\n",
      "step 100: train loss 0.3153, val loss 0.3152, acc val 0.8656\n",
      "step 101: train loss 0.3645, val loss 0.3586, acc val 0.8534\n",
      "step 102: train loss 0.2786, val loss 0.2840, acc val 0.8779\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 103: train loss 0.3263, val loss 0.3196, acc val 0.8683\n",
      "step 104: train loss 0.3672, val loss 0.3664, acc val 0.8406\n",
      "step 105: train loss 0.3100, val loss 0.3072, acc val 0.8752\n",
      "step 106: train loss 0.3378, val loss 0.3350, acc val 0.8603\n",
      "step 107: train loss 0.3223, val loss 0.3373, acc val 0.8584\n",
      "step 108: train loss 0.3396, val loss 0.3386, acc val 0.8521\n",
      "step 109: train loss 0.3078, val loss 0.3168, acc val 0.8730\n",
      "step 110: train loss 0.3064, val loss 0.3005, acc val 0.8742\n",
      "step 111: train loss 0.3484, val loss 0.3457, acc val 0.8587\n",
      "step 112: train loss 0.3841, val loss 0.3821, acc val 0.8647\n",
      "step 113: train loss 0.3386, val loss 0.3254, acc val 0.8621\n",
      "step 114: train loss 0.2936, val loss 0.2990, acc val 0.8749\n",
      "step 115: train loss 0.2899, val loss 0.2891, acc val 0.8893\n",
      "step 116: train loss 0.3356, val loss 0.3390, acc val 0.8656\n",
      "step 117: train loss 0.2772, val loss 0.2739, acc val 0.8894\n",
      "saving checkpoint to out/gpt-classifier\n",
      "step 118: train loss 0.2899, val loss 0.2853, acc val 0.8806\n",
      "step 119: train loss 0.3012, val loss 0.3012, acc val 0.8757\n",
      "Final result: train loss 0.3012, val loss 0.3012, acc val 0.8757\n"
     ]
    }
   ],
   "source": [
    "if wandb_log:\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
    "\n",
    "lossi_train = []\n",
    "lossi_val = []\n",
    "track_acc = []\n",
    "best_val_loss = 1e9\n",
    "\n",
    "for step in range(max_iter):\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        y_pred = clf(xb)\n",
    "        loss = loss_fn(y_pred, yb)\n",
    "\n",
    "        # compute the l1 penalty error term\n",
    "        #params = torch.cat([p.view(-1) for p in clf.lm_head.parameters()])\n",
    "        #l1_reg = lambda_1 * torch.norm(params, 1)\n",
    "        #loss += l1_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        lossi_train.append(estimate_loss(train_loader, return_acc=False))\n",
    "        loss_val, acc_val = estimate_loss(val_loader, return_acc=True)\n",
    "        lossi_val.append(loss_val)\n",
    "        track_acc.append(acc_val)\n",
    "        print(f\"step {step}: train loss {lossi_train[-1]:.4f}, val loss {lossi_val[-1]:.4f}, acc val {track_acc[-1]:.4f}\")\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": step,\n",
    "                \"train/loss\": lossi_train[-1],\n",
    "                \"val/loss\": lossi_val[-1],\n",
    "                \"val/acc\": track_acc[-1],\n",
    "                \"lr\": lr,\n",
    "                })\n",
    "\n",
    "        if lossi_val[-1] < best_val_loss:\n",
    "            best_val_loss = lossi_val[-1]\n",
    "            checkpoint = {\n",
    "                'model': clf.state_dict(),\n",
    "                'backbone': model,  # para inicializar la tabla de embedding del modelo gpt\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model_args': model_args,\n",
    "                'iter_num': step,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'config': config,\n",
    "                'gpt_meta': meta,\n",
    "             }\n",
    "            print(f\"saving checkpoint to {out}\")\n",
    "            torch.save(checkpoint, os.path.join(out, 'ckpt.pt'))\n",
    "\n",
    "    if warmup_iter and (step+1) == warmup_iter:\n",
    "        for p in clf.embedding_from_gpt.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "print(f\"Final result: train loss {lossi_train[-1]:.4f}, val loss {lossi_val[-1]:.4f}, acc val {track_acc[-1]:.4f}\")\n",
    "\n",
    "preds, targets = collect_preds(clf, val_loader)\n",
    "print(classification_report(preds.argmax(dim=1).numpy(), targets.numpy(), \n",
    "                            target_names=['normal', 'incivilidad', 'odio']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.87      0.86      0.86       459\n",
      " incivilidad       0.94      0.89      0.91       537\n",
      "        odio       0.72      0.82      0.77       225\n",
      "\n",
      "    accuracy                           0.86      1221\n",
      "   macro avg       0.84      0.85      0.85      1221\n",
      "weighted avg       0.87      0.86      0.87      1221\n",
      "\n",
      "[[393  19  47]\n",
      " [ 34 478  25]\n",
      " [ 27  14 184]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "preds, targets = collect_preds(clf, val_loader)\n",
    "print(classification_report(preds.argmax(dim=1).numpy(), targets.numpy(), \n",
    "                            target_names=['normal', 'incivilidad', 'odio']))\n",
    "\n",
    "print(confusion_matrix(preds.argmax(dim=1).numpy(), targets.numpy()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescataremos las probabilidades de predicción para cada una de las clases,\n",
    "y las verdaderas etiquetas para todo el conjunto de validación. Luego,\n",
    "evaluamos según las funciones de la competencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset: 1221\n",
      "Matriz de confusión\n",
      "[[393  27  34]\n",
      " [ 47 184  25]\n",
      " [ 19  14 478]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.86      0.87      0.86       454\n",
      "        odio       0.82      0.72      0.77       256\n",
      " incivilidad       0.89      0.94      0.91       511\n",
      "\n",
      "    accuracy                           0.86      1221\n",
      "   macro avg       0.85      0.84      0.85      1221\n",
      "weighted avg       0.86      0.86      0.86      1221\n",
      "\n",
      "Métricas:\n",
      "\n",
      "AUC:  0.957\tKappa: 0.787\tAccuracy: 0.864\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.957, 0.787, 0.864])"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import evaluate\n",
    "\n",
    "preds, targets = collect_preds(clf, val_loader)\n",
    "print(f\"Tamaño del dataset: {preds.shape[0]}\")\n",
    "pred_prob = F.softmax(preds.cpu(), dim=1).detach().numpy()\n",
    "\n",
    "y_idx = targets.cpu().numpy()\n",
    "y_label = np.array([dataset._id2label[x] for x in y_idx], dtype=\"object\")\n",
    "evaluate(pred_prob, y_label, np.array(list(dataset._label2id.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8183,), (4031,), (8183,), (4031,))"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=42\n",
    "\n",
    "def get_subsets(df):\n",
    "    return train_test_split(\n",
    "        df['texto'],\n",
    "        df['clase'],\n",
    "        shuffle=True,\n",
    "        test_size=0.33,\n",
    "        random_state=SEED,\n",
    "        stratify=df['clase']\n",
    "    )\n",
    "\n",
    "Xtr, Xval, Ytr, Yval = get_subsets(train_df)\n",
    "Xtr.shape, Xval.shape, Ytr.shape, Yval.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un nuevo loader...a partir de `Xval` y `Yval` de arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X = torch.zeros((Xval.shape[0], dataset.max_char), dtype=torch.long)\n",
    "\n",
    "for i, text in enumerate(Xval):\n",
    "    X[i, :len(text)] = torch.tensor(dataset.encode_fn(text))\n",
    "\n",
    "y = torch.tensor([dataset._label2id[x] for x in Yval], dtype=torch.long)\n",
    "test_this = TensorDataset(X, y)\n",
    "new_loader = DataLoader(test_this, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset: 4031\n",
      "Matriz de confusión\n",
      "[[1261   81   71]\n",
      " [ 122  632   74]\n",
      " [ 116   50 1624]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.84      0.89      0.87      1413\n",
      "        odio       0.83      0.76      0.79       828\n",
      " incivilidad       0.92      0.91      0.91      1790\n",
      "\n",
      "    accuracy                           0.87      4031\n",
      "   macro avg       0.86      0.85      0.86      4031\n",
      "weighted avg       0.87      0.87      0.87      4031\n",
      "\n",
      "Métricas:\n",
      "\n",
      "AUC:  0.962\tKappa: 0.799\tAccuracy: 0.872\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.962, 0.799, 0.872])"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, targets = collect_preds(clf, new_loader)\n",
    "print(f\"Tamaño del dataset: {preds.shape[0]}\")\n",
    "pred_prob = F.softmax(preds.cpu(), dim=1).detach().numpy()\n",
    "\n",
    "y_idx = targets.cpu().numpy()\n",
    "y_label = np.array([dataset._id2label[x] for x in y_idx], dtype=\"object\")\n",
    "evaluate(pred_prob, y_label, np.array(list(dataset._label2id.keys())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar el modelo tenemos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(out, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "checkpoint_model_args = checkpoint['model_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'gpt_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/C992-E9FE/dev/nanoGPT/model-usage.ipynb Cell 71\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/C992-E9FE/dev/nanoGPT/model-usage.ipynb#Y210sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m GPTClassifier(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheckpoint_model_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'gpt_model'"
     ]
    }
   ],
   "source": [
    "test = GPTClassifier(**checkpoint_model_args)\n",
    "state_dict = checkpoint['model']\n",
    "test.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTClassifier(\n",
       "  (embedding_from_gpt): Embedding(656, 384, padding_idx=0)\n",
       "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "  (hidden_layer): Linear(in_features=499200, out_features=16, bias=True)\n",
       "  (hidden_layer2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (lm_head): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.load_state_dict(clf.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTClassifier(\n",
       "  (embedding_from_gpt): Embedding(656, 384, padding_idx=0)\n",
       "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       "  (hidden_layer): Linear(in_features=499200, out_features=16, bias=True)\n",
       "  (hidden_layer2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (lm_head): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset: 1221\n",
      "Matriz de confusión\n",
      "[[393  27  34]\n",
      " [ 47 184  25]\n",
      " [ 19  14 478]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.86      0.87      0.86       454\n",
      "        odio       0.82      0.72      0.77       256\n",
      " incivilidad       0.89      0.94      0.91       511\n",
      "\n",
      "    accuracy                           0.86      1221\n",
      "   macro avg       0.85      0.84      0.85      1221\n",
      "weighted avg       0.86      0.86      0.86      1221\n",
      "\n",
      "Métricas:\n",
      "\n",
      "AUC:  0.957\tKappa: 0.787\tAccuracy: 0.864\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.957, 0.787, 0.864])"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import evaluate\n",
    "\n",
    "test.to(device)\n",
    "\n",
    "preds, targets = collect_preds(test, val_loader)\n",
    "print(f\"Tamaño del dataset: {preds.shape[0]}\")\n",
    "pred_prob = F.softmax(preds.cpu(), dim=1).detach().numpy()\n",
    "\n",
    "y_idx = targets.cpu().numpy()\n",
    "y_label = np.array([dataset._id2label[x] for x in y_idx], dtype=\"object\")\n",
    "evaluate(pred_prob, y_label, np.array(list(dataset._label2id.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
