{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar modelo pre-entrenado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out'\n",
    "start = ''\n",
    "num_samples = 10\n",
    "max_new_tokens = 500\n",
    "temperature = 0.9\n",
    "#top_k = 200\n",
    "seed = 33313988\n",
    "device = 'cuda'\n",
    "dtype='float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar configuraciones del checkpoint e inicializarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la configuraciÃ³n del Ãºltimo checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=128, vocab_size=526, n_layer=3, n_head=6, n_embd=384, dropout=0.0, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptconf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(526, 384)\n",
       "  (position_embedding_table): Embedding(128, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (lm_head): Linear(in_features=384, out_features=526, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NÃºmero de parÃ¡metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÃºmero de parÃ¡metros GPT-sentiment checkpoint: 5.52 millones\n"
     ]
    }
   ],
   "source": [
    "print(f\"NÃºmero de parÃ¡metros GPT-sentiment checkpoint: {model.get_num_params()/1e6:.2f} millones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar tokenizador y crear funciones `encode` y `decode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with open('./data/by_char/meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    itos = meta['itos']\n",
    "    stoi = meta['stoi']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneraciÃ³n de muestras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear una funciÃ³n para generar muestras y concatenarlas.\n",
    "\n",
    "Esta funciÃ³n se utiliza en `train.py` para capturar samples en \n",
    "cada loop de evaluaciÃ³n del modelo. La idea es ver como evoluciona\n",
    "la generaciÃ³n de texto durante el entrenamiento, y complementar la\n",
    "informaciÃ³n de la funciÃ³n de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_samples(num_samples, max_new_tokens=10):\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for k in range(num_samples):\n",
    "            y = model.generate(idx=torch.zeros((1, 128), dtype=torch.long, device=device), max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "            out.append(f\"({k+1}) {decode(y[0][128:].tolist())}\")\n",
    "    model.train()\n",
    "    return '\\n'.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1)                     asueltos ?  ondiarte no si no, solo hay carter leÃ­, un hombre 1 famos inflar del descuento y arriesinvierno no se debe ser tratadas por decir de lo que lo dijo esa huello es de #senansventonces https://t.co/MXpdjlxcWN\"@user Por co\n",
      "(2)   @user         Alguien!!!!     @user ...@user Si super partimado ctm el igual que ellos arrÃ¡Ã±o en trans malos en la locurra de segundar alfartos y extranjeros todos los dÃ­as indios y se me inefasto recrecto con puesto de mierda.Esta gente mÃ¡s corrup\n",
      "(3)                  htmerosMe hace me sosto a votar cuando... q como respreciÃ³ a los chilenos..en la misma votomada... Ahora estÃ¡s el potito no sexual de la oportunidad con negra a esta sociedad...contribuyen de programas tienen ðŸ¤£ https://t.co/A ZWwlqlb\n",
      "(4)   ðŸ˜‚NT ðŸ‘©â€[6/22221    ðŸ”¹âœ”ï¸ðŸ¤£ https://t.co/xP6fpZC469@user Segunda   Me lastre ctm, hasta homosexual, como se puede ir incluso de jÃ³venes segÃºn putas de verdaderos a madre y muchos ! ðŸ™ŒðŸ”µ[Listos sigan iguales se va a la semana de toda la presidencia de floj\n",
      "(5) @user   wn ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚  Â¿? A veo?? \"\"@user            \"\"Ã‘E MUJER       Indio a Floritar ser ignorantes ni saldrÃ­a hermosura.    Solos es que si el eres reen la pilaratada 1 20 minutos emigrantes por la justicia policÃ­a callate para daÃ±o y su candidata\n",
      "(6)                     \"Hay grae   es lo que tenemos feliz la CHUCHA@user me hasta te peso qliao no significa se hizÃ³ le pelÃ­cula tiene que ser cierto los comunistas en la pedazos de mi puta https://t.co/S0BUB8bRdMeni @user a la llegar a las mujeres neg\n",
      "(7) ....\"\" \"\"sabe  \"\" https://t.co/BthQUwR3Y aweonaSi qliao fue tener Morir, es, 13, es de cachito no en tus harrorizos del fÃºtbol y las mujeres no tienen no entendimos se vagan para hacer con su casa@user @user Parece claro y la van a weona que te da ve\n",
      "(8)        paÃ±eras@user @user @user @user @user @user ve delincuentes de que no Godric no es para declarar con la culencia por supuesto de cubes.@user SerÃ­a presidente a un servicio ðŸš«/7lo bien quinte de atrÃ¡s restar que creen sus propioridades estÃ¡n dete\n",
      "(9)       Â¡Â¡ya hasta una mejora que pasa la soluja lacras con el paÃ­s! No se escuchan sondomiales y La Delgadez y superte maraco chaque ðŸ˜…ðŸ˜….una amenaza y los del narco, pues sin dudan hombres y la polÃ­tica sola como un bitromano es caso a la sociedad.Hola\n",
      "(10)               A       Ean Ã‘enutil #Botis ctm, esto    Y huevo en el igual que dije, que no deja me puta y estos estricos celebrar , antes que los niÃ±os putas https://t.co/MpQps5IyA\"La culpa de ferminas trans cuides a les normal, si deseaba que hay te\n",
      "(11)                          MÃ©xica es en Swechell pero rotella serie u vieja ctm qliao tildras largas de lo mil de otros, dejamos bastados apoyarÃ­an un salir mÃ¡s importante.@user @user Psinocita. jajajajajajajajajaja !!!!! https://t.co/4bDCHqs3hZHos ven\n",
      "(12) J https://t.co/M2lIqfiCj8@user @user @user @user @user \"@user @user @user Noo me puede cadera mujer @user@user @user @user @user Si Obvio, todo el amor con manos parte el mundo era de la locura de los poderos. Una mujer buts que Los negros jÃ³venes de\n",
      "(13)        A TO LOBBORDEFFP.  FUE MÃS IBERES CTM ðŸ¤”ðŸ¤”ðŸ¤”pirensÃ© la maravilla puedes no tenemos un puto enojado de una sueÃ±a reclamando 50 seminadas y Manes de la adversie, internador en la plaza no haber en ninguna ex fuente huea su sicidente. Tampoco nunca \n",
      "(14)           . A Stgres \"\"Bueno tiene pol el 71% del SenarvÃ­a El hombre . https://t.co/aAW7zZ3GT5@user @user @user @user @user Una regionario no existe el narcotrafico y lo mÃ¡s del tins, no pueden ser profundos cualquier mega, acÃ¡ conversabispero a una \n",
      "(15)                                   Se sebes aproveeen a los saldrÃ¡s psde alguien cuenta en su solla lados. @user @user Porque esta aportador a nosotros ponen KarinaOlive bille y jajajajajajaAAa Asquiciar en vez promedio en Funciona Biserof con la verd\n",
      "(16)    #Migna #Siguen     #supremiones   TENDES@user @user @user @user Le cabes con bajas y existen. Nacionalizar la razÃ³n del mal #dilwnsapes instalan venezolanos y los otros negros #Olivelar, que la historiaQiete pero no se vaya a saliendo somos patÃ©ti\n",
      "(17)     #BTVNDL   #PDTðŸ¤¬pPA TO es zurdo la hizoTerrico Ufd en mi perra jajajajjajajajajajajajajajajajajajaEExplotay que demuestra mujer, me diria. https://t.co/h1qqlTfVtmv@user Pero si es dos condiciones a las personas honradas que no te insegura que pued\n",
      "(18)         agracias y weay femina el pueblo 214 una la prensa , que la cara es el hijo de los pueblos sur todo en vez de mierda@user Jajajajajajaajjajaja te disfrir votad por perder @user https://t.co/eewHiewPs@user De que me gusta el mela colgadesperso\n",
      "(19) , y mi jujajajakw Buen y quera persona @user@user @user @user Tante ctm ðŸ¤£ðŸ¤£ðŸ¤£@user @user @user @user @user @user @user @user @user @user @user @user @user @user Si pero piensa en promischilenos son una amenaza para el mujer, son muy religiados. Q arrog\n",
      "(20)         En el creerse â€œchillaâ€ en la capitÃ¡ en ChillÃ¡les de marejar, ni es en clasista, el esadito lada ? #KarinaOlivaGolaivano en la selecciÃ³n a ministro de los formalos y violentos (de polÃ­ticos).@user @user Hasta que han preguntado a la tener una \n"
     ]
    }
   ],
   "source": [
    "print(get_samples(20, max_new_tokens=250))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo:** user modelo GPT-sentiment pre-entrenado y agregar una\n",
    "cabeza de clasificaciÃ³n para adaptar la representaciÃ³n del texto\n",
    "a discriminar a que clase pertenece cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This model is a simple MLP with one embedding layer with the weights of the Llama model.\n",
    "    \"\"\"\n",
    "    def __init__(self, gpt_backbone, n_classes=3, freeze=True):\n",
    "        super(GPTClassifier, self).__init__()\n",
    "        self.gpt_backbone = gpt_backbone\n",
    "\n",
    "        # freeze parameters of the gpt backbone\n",
    "        if freeze:\n",
    "            for param in self.gpt_backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # add new classification head\n",
    "        self.gpt_backbone.lm_head = nn.Linear(gpt_backbone.lm_head.in_features, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gpt_backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva cÃ¡beza del modelo ---> Linear(in_features=384, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "clf = GPTClassifier(model, n_classes=3, freeze=True)\n",
    "print(f\"Nueva cÃ¡beza del modelo ---> {clf.gpt_backbone.lm_head}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparar los datos y verificar que fluyan correctamente por el modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
